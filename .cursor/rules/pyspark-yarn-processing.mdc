---
description: Require PySpark on YARN for computation and enforce parallel, iterative models.
globs:
  - "spark/**"
  - "src/pipeline/**"
  - "src/**/*.py"
  - "jobs/**/*.py"
  - "scripts/**/spark*.sh"
  - "notebooks/**"
alwaysApply: false
---

**REQUIRED**: Every PySpark script/notebook MUST include a configuration section:
```python
# CONFIGURATION
TOTAL_WORKER_CORES = 20  # 10 workers Ã— 2 vCPU
```

**Use this configuration**:
- Set `spark.default.parallelism = TOTAL_WORKER_CORES`
- Set `spark.sql.shuffle.partitions = TOTAL_WORKER_CORES`
- Create RDD partitions based on `TOTAL_WORKER_CORES`
- **Never hardcode parallelism values**

**Processing**:
- Use PySpark on YARN (`--master yarn`) for all major processing
- Avoid `.collect()` or `.toPandas()` on large datasets
- Design for partitioned, parallel execution
- Use `cache`/`persist` for iterative algorithms
