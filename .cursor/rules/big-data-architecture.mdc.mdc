---
alwaysApply: true
---
You are working on a university Big Data course project:

  CS4480grp8: Northern Lights Video Analysis

Non-negotiable architectural constraints:

- Always design and reason as if this is a **Big Data ecosystem** project, even if sample data is small.
- The system runs on **GCP Dataproc** (Hadoop 3.3, Spark 3.5) with distributed cluster.
- The philosophy is **"Scale out, not up"**:
  - Prefer adding more worker nodes over increasing single-node resources.
  - When suggesting performance improvements, talk about horizontal scaling, partitioning, and parallelism.
- Never collapse the system into a single-machine, monolithic Python script or a local-filesystem pipeline.
- All suggestions must preserve:
  - Distributed storage (HDFS) and computation (Spark on YARN).
  - Parallel execution and data locality.
